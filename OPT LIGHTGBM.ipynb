{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7513098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "import gc\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "import itertools\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import hp, fmin, tpe\n",
    "#from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9ab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ==================================================== \n",
    "def read_data():\n",
    "    train = pd.read_parquet(\"./train_fe_plus_plus.parquet\")\n",
    "    test = pd.read_parquet(\"./test_fe_plus_plus.parquet\")\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d0b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbcb9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "    动态回调参数函数，params视作字典\n",
    "    :param params:lgb参数字典\n",
    "    :return params:修正后的lgb参数字典\n",
    "    \"\"\"\n",
    "    params['feature_pre_filter'] = False\n",
    "    params['objective'] = 'binary'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    #params['bagging_seed'] = 2020\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e3e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_hyperopt(train):\n",
    "    \"\"\"\n",
    "    模型参数搜索与优化函数\n",
    "    :param train:训练数据集\n",
    "    :return params_best:lgb最优参数\n",
    "    \"\"\"\n",
    "    # Part 1.划分特征名称，删除ID列和标签列\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('customer_ID')\n",
    "    features.remove('target')\n",
    "    \n",
    "    # Part 2.封装训练数据\n",
    "    train_data = lgb.Dataset(train[features], train[label])\n",
    "    \n",
    "    # Part 3.内部函数，输入模型超参数损失值输出函数\n",
    "    def hyperopt_objective(params):\n",
    "        \"\"\"\n",
    "        输入超参数，输出对应损失值\n",
    "        :param params:\n",
    "        :return:最小rmse\n",
    "        \"\"\"\n",
    "        # 创建参数集\n",
    "        params = params_append(params)\n",
    "        print(params)\n",
    "        \n",
    "        # 借助lgb的cv过程，输出某一组超参数下损失值的最小值\n",
    "        res = lgb.cv(params, train_data, 1000,\n",
    "                     nfold=2,\n",
    "                     stratified=False,\n",
    "                     shuffle=True,\n",
    "                     metrics='rmse',\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose_eval=False,\n",
    "                     show_stdv=False,\n",
    "                     seed=2020)\n",
    "        return min(res['rmse-mean']) # res是个字典\n",
    "\n",
    "    # Part 4.lgb超参数空间\n",
    "    params_space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "        'num_leaves': hp.choice('num_leaves', list(range(10, 300, 10))),\n",
    "        'reg_alpha': hp.randint('reg_alpha', 0, 10),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "        'bagging_freq': hp.randint('bagging_freq', 1, 10),\n",
    "        'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Part 5.TPE超参数搜索\n",
    "    params_best = fmin(\n",
    "        hyperopt_objective,\n",
    "        space=params_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        rstate=np.random.default_rng(2020))\n",
    "        #rstate=RandomState(2020))\n",
    "    \n",
    "    # 返回最佳参数\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f5351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.9429104308567877, 'bagging_freq': 2, 'feature_fraction': 0.5715782198140802, 'learning_rate': 0.21315219327595428, 'min_child_samples': 11, 'num_leaves': 160, 'reg_alpha': 3, 'reg_lambda': 7.561160634893758, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.491737 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.601891 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.512262561313273, 'bagging_freq': 3, 'feature_fraction': 0.6864235320941958, 'learning_rate': 0.31527024380943564, 'min_child_samples': 1, 'num_leaves': 180, 'reg_alpha': 2, 'reg_lambda': 7.569488156706784, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.927268 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.089927 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.8893476235428235, 'bagging_freq': 7, 'feature_fraction': 0.7135664981551813, 'learning_rate': 0.2903641143253666, 'min_child_samples': 21, 'num_leaves': 210, 'reg_alpha': 8, 'reg_lambda': 5.393157199547058, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.129792 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.908321 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.6787796182704178, 'bagging_freq': 1, 'feature_fraction': 0.7013446622253534, 'learning_rate': 0.4739191471915275, 'min_child_samples': 21, 'num_leaves': 190, 'reg_alpha': 0, 'reg_lambda': 7.368279836937887, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.927225 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.954385 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.7626783188810151, 'bagging_freq': 6, 'feature_fraction': 0.8391402503316614, 'learning_rate': 0.36741303605044506, 'min_child_samples': 26, 'num_leaves': 40, 'reg_alpha': 4, 'reg_lambda': 9.629414851907006, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.949951 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.944444 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.7897880317843915, 'bagging_freq': 1, 'feature_fraction': 0.6864530344942663, 'learning_rate': 0.16801009574165351, 'min_child_samples': 6, 'num_leaves': 160, 'reg_alpha': 6, 'reg_lambda': 7.928417982876395, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.920161 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.928359 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.5709455293113112, 'bagging_freq': 6, 'feature_fraction': 0.6817716125366499, 'learning_rate': 0.28149790303677774, 'min_child_samples': 26, 'num_leaves': 50, 'reg_alpha': 2, 'reg_lambda': 0.9202721245005685, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.960908 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.039467 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.8894268166893926, 'bagging_freq': 7, 'feature_fraction': 0.8614864228571265, 'learning_rate': 0.10838878622896095, 'min_child_samples': 16, 'num_leaves': 70, 'reg_alpha': 7, 'reg_lambda': 4.128216047488188, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.946488 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.989105 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.8689240389616162, 'bagging_freq': 6, 'feature_fraction': 0.8839374526657198, 'learning_rate': 0.1295409886684552, 'min_child_samples': 1, 'num_leaves': 120, 'reg_alpha': 1, 'reg_lambda': 0.8934485689740768, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.894629 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.976214 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.9827696256425138, 'bagging_freq': 6, 'feature_fraction': 0.6241629598983252, 'learning_rate': 0.2693431830624993, 'min_child_samples': 11, 'num_leaves': 30, 'reg_alpha': 7, 'reg_lambda': 5.653589408911715, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.946662 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.932675 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.5890956497689592, 'bagging_freq': 8, 'feature_fraction': 0.9337630517879247, 'learning_rate': 0.1227160059413021, 'min_child_samples': 6, 'num_leaves': 220, 'reg_alpha': 2, 'reg_lambda': 0.31991676687450954, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.988260 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.964786 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258830 -> initscore=-1.052061                                        \n",
      "[LightGBM] [Info] Start training from score -1.052061                                                                  \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259034 -> initscore=-1.050993                                        \n",
      "[LightGBM] [Info] Start training from score -1.050993                                                                  \n",
      "{'bagging_fraction': 0.8729748815879685, 'bagging_freq': 2, 'feature_fraction': 0.6729872738023669, 'learning_rate': 0.28953159858971855, 'min_child_samples': 1, 'num_leaves': 250, 'reg_alpha': 9, 'reg_lambda': 4.085962466064952, 'feature_pre_filter': False, 'objective': 'binary', 'metric': 'binary_logloss'}\n",
      "[LightGBM] [Info] Number of positive: 59390, number of negative: 170066                                                \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.064396 seconds.             \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255446                                                                                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1636                        \n",
      "[LightGBM] [Info] Number of positive: 59437, number of negative: 170019                                                \n",
      " 37%|█████████████████▏                             | 11/30 [29:07<52:54, 167.08s/trial, best loss: 0.2624819291100847]"
     ]
    }
   ],
   "source": [
    "best_clf = param_hyperopt(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234f65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "import gc\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "import itertools\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = \"./\"\n",
    "    seed = 53\n",
    "    n_folds = 5\n",
    "    target = \"target\"\n",
    "    boosting_type = \"53-5-88000-22000-0.003\"\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + \"train_diff.parquet\")\n",
    "    test = pd.read_parquet(CFG.input_dir + \"test_diff.parquet\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:, 0] == 0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:, 0]) / np.sum(labels[:, 0])\n",
    "    gini = [0, 0]\n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:, 0] == 0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] * weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1] / gini[0] + top_four)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "class DartEarlyStopping(object):\n",
    "    def __init__(self, data_name, monitor_metric, stopping_round):\n",
    "        self.data_name = data_name\n",
    "        self.monitor_metric = monitor_metric\n",
    "        self.stopping_round = stopping_round\n",
    "        self.best_score = None\n",
    "        self.best_model = None\n",
    "        self.best_score_list = []\n",
    "        self.best_iter = 0\n",
    "\n",
    "    def _is_higher_score(self, metric_score, is_higher_better):\n",
    "        if self.best_score is None:\n",
    "            return True\n",
    "        return (self.best_score < metric_score) if is_higher_better else (self.best_score > metric_score)\n",
    "\n",
    "    def _deepcopy(self, x):\n",
    "        return pickle.loads(pickle.dumps(x))\n",
    "\n",
    "    def __call__(self, env):\n",
    "        evals = env.evaluation_result_list\n",
    "        for data, metric, score, is_higher_better in evals:\n",
    "            if data != self.data_name or metric != self.monitor_metric:\n",
    "                continue\n",
    "            if not self._is_higher_score(score, is_higher_better):\n",
    "                if env.iteration - self.best_iter > self.stopping_round:\n",
    "                    eval_result_str = '\\t'.join([lgb.callback._format_eval_result(x) for x in self.best_score_list])\n",
    "                    print(f\"Early stopping, best iteration is:\\n[{self.best_iter+1}]\\t{eval_result_str}\") \n",
    "                    print(f\"You can get best model by \\\"DartEarlyStopping.best_model\\\"\")\n",
    "                    raise lgb.callback.EarlyStopException(self.best_iter, self.best_score_list)\n",
    "                return\n",
    "\n",
    "            self.best_model = self._deepcopy(env.model)\n",
    "            self.best_iter = env.iteration\n",
    "            self.best_score_list = evals\n",
    "            self.best_score = score\n",
    "            return\n",
    "        raise ValueError(\"monitoring metric not found\")\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(\n",
    "        train.dtypes[(train.dtypes == \"float32\") | (train.dtypes == \"float64\")].index\n",
    "    )\n",
    "    num_cols = [col for col in num_cols if \"last\" in col]\n",
    "    for col in num_cols:\n",
    "        train[col + \"_round2\"] = train[col].round(2)\n",
    "        test[col + \"_round2\"] = test[col].round(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "        \n",
    "        \n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in [\"customer_ID\", CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.003,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(\" \")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Training fold {fold} with {len(features)} features...\")\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "#        y_train, y_val = (\n",
    "#            train[CFG.target].iloc[trn_ind],\n",
    "#            train[CFG.target].iloc[val_ind],\n",
    "#        )\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "\n",
    "        es = DartEarlyStopping(\"valid_0\", \"amex_metric\", stopping_round=22000)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=88000,\n",
    "            valid_sets=[lgb_valid],\n",
    "            early_stopping_rounds=22000,\n",
    "            callbacks=[es],\n",
    "            verbose_eval=50,\n",
    "            feval=lgb_amex_metric,\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        joblib.dump(\n",
    "            model,\n",
    "            f\"lgbm_fold{fold}_{CFG.boosting_type}_seed{CFG.seed}.pkl\",\n",
    "        )\n",
    "        # Predict validation\n",
    "        print(\"ready to Predict validation\")\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        print(\"ready to Add to out of folds array\")\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        print(\"ready to Predict the test set\")\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f\"Our fold {fold} CV score is {score}\")\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    \n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f\"Our out of folds CV score is {score}\")\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "#    oof_df = pd.DataFrame(\n",
    "#        {\n",
    "#            \"customer_ID\": train[\"customer_ID\"],\n",
    "#            \"target\": train[CFG.target],\n",
    "#            \"prediction\": oof_predictions,\n",
    "#        }\n",
    "#    )\n",
    "#    oof_df.to_csv(\n",
    "#        f\"oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv\",\n",
    "#        index=False,\n",
    "#    )\n",
    "    # Create a dataframe to store test prediction\n",
    "#    test_df = pd.DataFrame(\n",
    "#        {\"customer_ID\": test[\"customer_ID\"], \"prediction\": test_predictions}\n",
    "#    )\n",
    "#    test_df.to_csv(\n",
    "#        f\"test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv\",\n",
    "#        index=False,\n",
    "#    )\n",
    "\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'modifioof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'modifitest_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
